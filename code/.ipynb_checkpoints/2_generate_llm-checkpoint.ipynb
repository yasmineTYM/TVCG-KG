{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029976eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/yameitu/Documents/Yamei/Research/2024PVIS/python_code/')\n",
    "import utils\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Any, Callable, List, Optional, Sequence, Type, Union, cast\n",
    "from kor.extraction import extract_from_documents\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from datetime import datetime\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634f690a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/1_tvcg_with_abstract_4987.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/1_tvcg_with_abstract_4987.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Yamei/Research/2024PVIS/python_code/utils.py:22\u001b[0m, in \u001b[0;36mloadjson\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadjson\u001b[39m(filepath):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Opening JSON file\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# returns JSON object as \u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# a dictionary\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/1_tvcg_with_abstract_4987.json'"
     ]
    }
   ],
   "source": [
    "data = utils.loadjson('../data/1_tvcg_with_abstract_4987.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29728cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_difference = []\n",
    "for i in range(len(data['tvcg'])):\n",
    "    if data['tvcg'][i]['data']['article']['abstract']!=data['tvcg'][i]['data']['article']['normalizedAbstract']:\n",
    "        has_difference.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9741ca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(has_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2adcb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. # Clean data \n",
    "# data = utils.loadjsonl('../tvcg_processed/0_all_papers_only_4987.jsonl')\n",
    "# not_null = list(filter(lambda d: d['data']['article']['normalizedAbstract']!=None, data))\n",
    "\n",
    "# print(len(data), len(not_null))\n",
    "\n",
    "# utils.savejson({\"tvcg\": not_null}, '../raw_data/1_tvcg_with_abstract_4987.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e53b6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(data[0])\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path = '../raw_data/1_tvcg_with_abstract_4987.json',\n",
    "    jq_schema = '.tvcg[].data.article.normalizedAbstract'\n",
    "    )\n",
    "data = loader.load()\n",
    "sequence_data: Sequence[Document] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94e6e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuyamei testing!!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def getSchemaFlat(example):\n",
    "    schema = Object(\n",
    "        id=\"scientific_concepts\",\n",
    "        attributes = [\n",
    "        Text(\n",
    "            id=\"task\",\n",
    "            description= \"the goal or task of the paper\",\n",
    "            many=True\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"application\",\n",
    "            description = \"the applied domain of the paper\",\n",
    "            many=True\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"name\",\n",
    "            description=\"name of proposed method, framework, algorithm of the paper\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"technique\",\n",
    "            description = \"techniques used in the method\",\n",
    "            many=True\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"input_data\",\n",
    "            description = \"input data type\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"output_data\",\n",
    "            description = \"output data type\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"evaluation_technique\",\n",
    "            description = \"evaluation method, metric\",\n",
    "            many=True\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"evaluation_baseline\",\n",
    "            description = \"baseline methods used for comparision purpose\",\n",
    "            many=True\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"evaluation_data\",\n",
    "            description = \"data or dataset used for evaluation\",\n",
    "            many=True\n",
    "        ),\n",
    "        \n",
    "            \n",
    "        Text(\n",
    "            id=\"source_code\",\n",
    "        )\n",
    "        ],\n",
    "        examples = example\n",
    "    )\n",
    "    return schema\n",
    "\n",
    "\n",
    "\n",
    "example_vr_flat = [(\"\"\"\n",
    "   Experiential learning (ExL) is the process of learning through experience or more specifically \n",
    "   “learning through reflection on doing”. In this paper, we propose a simulation of these experiences, \n",
    "   in Augmented Reality (AR), addressing the problem of language learning. \n",
    "   Such systems provide an excellent setting to support “adaptive guidance”, \n",
    "   in a digital form, within a real environment. \n",
    "   Adaptive guidance allows the instructions and learning content to be customised for the individual learner, \n",
    "   thus creating a unique learning experience. We developed an adaptive guidance AR system for language learning, \n",
    "   we call Arigatō (Augmented Reality Instructional Guidance & Tailored Omniverse), \n",
    "   which offers immediate assistance, resources specific to the learner's needs, \n",
    "   manipulation of these resources, and relevant feedback. \n",
    "   Considering guidance, we employ this prototype to investigate the effect of the amount of guidance \n",
    "   (fixed vs. adaptive-amount) and the type of guidance (fixed vs. adaptive-associations) \n",
    "   on the engagement and consequently the learning outcomes of language learning in an AR environment. \n",
    "   The results for the amount of guidance show that compared to the adaptive-amount, \n",
    "   the fixed-amount of guidance group scored better in the immediate and delayed (after 7 days) recall tests. \n",
    "   However, this group also invested a significantly higher mental effort to complete the task. \n",
    "   The results for the type of guidance show that the adaptive-associations group outperforms the\n",
    "   fixed-associations group in the immediate, delayed (after 7 days) recall tests, and learning efficiency. \n",
    "   The adaptive-associations group also showed significantly lower mental effort and spent less time \n",
    "   to complete the task.\"\"\",\n",
    "   {\n",
    "        \n",
    "            \"task\": \"Experiential Learning (ExL)\",\n",
    "            \"application\": \"Augemented Reality\",\n",
    "            \"name\": \"Arigatō (Augmented Reality Instructional Guidance & Tailored Omniverse)\",\n",
    "            \"technique\": [\n",
    "                  \"adaptive guidence AR system\",\n",
    "                  \"guidance investigation\",\n",
    "                  \"immediate assistance\",\n",
    "                  \"learner-specific resources\",\n",
    "                  \"resource manipulation\",\n",
    "                  \"relevant feedback\"\n",
    "              ],\n",
    "\n",
    "          \"target\": [\"adaptive-association\",\"fixed-amount of guidance\"],\n",
    "          \"method\": [\"recall test\", \"learning efficiency\",'mental effort','completion time',\"recall test\"],\n",
    "          \"baseline\": [\"fixed-association\",\"adaptive-amount of guidance\"]\n",
    "    }\n",
    ")]\n",
    "example_vast_flat = [\n",
    "            (\n",
    "            \"\"\"\n",
    "            A systematic review (SR) is essential with up-to-date research evidence to support clinical decisions \n",
    "            and practices. However, the growing literature volume makes it challenging for SR reviewers and \n",
    "            clinicians to discover useful information efficiently. Many human-in-the-loop information retrieval \n",
    "            approaches (HIR) have been proposed to rank documents semantically similar to users  \n",
    "            queries and provide interactive visualizations to facilitate document retrieval. \n",
    "            Given that the queries are mainly composed of keywords and keyphrases retrieving \n",
    "            documents that are semantically similar to a query does not necessarily respond to \n",
    "            the clinician s need. Clinicians still have to review many documents to find the solution. \n",
    "            The problem motivates us to develop a visual analytics system, DocFlow, to facilitate information-seeking. \n",
    "            One of the features of our DocFlow is accepting natural language questions. The detailed description \n",
    "            enables retrieving documents that can answer users  questions. Additionally, clinicians \n",
    "            often categorize documents based on their backgrounds and with different purposes \n",
    "            (e.g., populations, treatments). Since the criteria are unknown and cannot \n",
    "            be pre-defined in advance, existing methods can only achieve categorization by \n",
    "            considering the entire information in documents. In contrast, by locating answers \n",
    "            in each document, our DocFlow can intelligently categorize documents based on users  \n",
    "            questions. The second feature of our DocFlow is a flexible interface where users can \n",
    "            arrange a sequence of questions to customize their rules for document retrieval and categorization. \n",
    "            The two features of this visual analytics system support a flexible information-seeking process. \n",
    "            The case studies and the feedback from domain experts demonstrate the usefulness and \n",
    "            effectiveness of our DocFlow.\n",
    "            \"\"\",\n",
    "                 {\n",
    "                     \"task\": [\"systematic review\"], \n",
    "                     \"application\": [\n",
    "                         \"human-in-the-loop information retrieval\", \n",
    "                         \"visual analytics\",\n",
    "                         \"clinical decisions and practices\"], \n",
    "                      \"name\": \"DocFlow\", \n",
    "                      \"technique\": [\n",
    "                          \"natural language processing\", \n",
    "                          \"semantic similarity ranking\", \n",
    "                          \"intelligent document categorization\", \n",
    "                          \"flexible interface\"], \n",
    "                      \"input\": \"keywords and keyphrases\", \n",
    "                      \"output\": \"relevant documents\", \n",
    "                      \"evaluation_technique\": [\n",
    "                          \"case study\",\n",
    "                          \"domain feedback\"\n",
    "                      ]\n",
    "               }\n",
    "\n",
    "            )\n",
    "        ]\n",
    "schema = getSchemaFlat(example_vast_flat+example_vr_flat)\n",
    "instruction_template = PromptTemplate(\n",
    "    input_variables=[\"format_instructions\", \"type_description\"],\n",
    "    template=(\n",
    "        \"Your goal is to extract, sumamrize or distill scientific concepts from abstracts of each publication.\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Outputs should folow the form described below.\\n\"\n",
    "        \"2. entities should matchs the type information exactly. Do not add any attribute that do not appear in the schema shown below.\\n\"\n",
    "        \"3. entities should be scientific concepts, usually phrases, less than 5 words.\\n\"\n",
    "        \"4. if the entity is a clause, try to distill the concepts from clause\"\n",
    "        \"{type_description}\\n\\n\"\n",
    "        \"{format_instructions}\\n\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "#     model_name = \"gpt-4\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    openai_api_key = \"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\",\n",
    ")\n",
    "chain = create_extraction_chain(llm, schema, instruction_template=instruction_template, encoder_or_encoder_class=\"json\")\n",
    "# chain = create_extraction_chain(llm, schema, encoder_or_encoder_class=\"json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f3ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = await extract_from_documents(chain=chain, documents=sequence_data, max_concurrency=1, use_uid=False )\n",
    "result_data3 = [ele['data'] for ele in result3]\n",
    "utils.savejsonl(result_data3, '../llm_parsed_data/on_new_dataset(complete)/model3.5_custom2_flat_vastvr_4987.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97d4914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"When point clouds are labeled in information visualization applications, sophisticated guidelines as in cartography do not yet exist. Existing naive strategies may mislead as to which points belong to which label. To inform improved strategies, we studied factors influencing this phenomenon. We derived a class of labeled point cloud representations from existing applications and we defined different models predicting how humans interpret such complex representations, focusing on their geometric properties. We conducted an empirical study, in which participants had to relate dots to labels in order to evaluate how well our models predict. Our results indicate that presence of point clusters, label size, and angle to the label have an effect on participants' judgment as well as that the distance measure types considered perform differently discouraging the use of label centers as reference points.\", metadata={'source': '/Users/yameitu/Documents/Yamei/Research/2024PVIS/raw_data/1_tvcg_with_abstract_4987.json', 'seq_num': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b33e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9305b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 752)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs), len(has_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "792ebb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "44\n",
      "77\n",
      "78\n",
      "80\n",
      "83\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "92\n",
      "93\n",
      "99\n",
      "116\n",
      "117\n",
      "120\n",
      "126\n",
      "128\n",
      "129\n",
      "136\n",
      "137\n",
      "140\n",
      "141\n",
      "143\n",
      "146\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "157\n",
      "159\n",
      "160\n",
      "162\n",
      "164\n",
      "174\n",
      "176\n",
      "179\n",
      "181\n",
      "183\n",
      "184\n",
      "189\n",
      "192\n",
      "193\n",
      "195\n",
      "200\n",
      "201\n",
      "204\n",
      "206\n",
      "207\n",
      "208\n",
      "212\n",
      "216\n",
      "217\n",
      "220\n",
      "221\n",
      "222\n",
      "225\n",
      "226\n",
      "240\n",
      "274\n",
      "299\n",
      "344\n",
      "346\n",
      "386\n",
      "387\n",
      "389\n",
      "427\n",
      "526\n",
      "567\n",
      "619\n",
      "627\n",
      "839\n",
      "860\n",
      "862\n",
      "897\n",
      "925\n",
      "928\n",
      "932\n",
      "934\n",
      "959\n",
      "985\n",
      "996\n",
      "998\n",
      "1002\n",
      "1009\n",
      "1019\n",
      "1047\n",
      "1048\n",
      "1050\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1066\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1077\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n",
      "1100\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1116\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1160\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1183\n",
      "1204\n",
      "1360\n",
      "1368\n",
      "1388\n",
      "1511\n",
      "1516\n",
      "1538\n",
      "1545\n",
      "1548\n",
      "1572\n",
      "1573\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1580\n",
      "1582\n",
      "1583\n",
      "1585\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1593\n",
      "1594\n",
      "1596\n",
      "1598\n",
      "1600\n",
      "1601\n",
      "1603\n",
      "1604\n",
      "1613\n",
      "1639\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1656\n",
      "1662\n",
      "1676\n",
      "1678\n",
      "1679\n",
      "1681\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1694\n",
      "1695\n",
      "1697\n",
      "1700\n",
      "1702\n",
      "1703\n",
      "1712\n",
      "1750\n",
      "1759\n",
      "1766\n",
      "1777\n",
      "1779\n",
      "1785\n",
      "1789\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1799\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1806\n",
      "1809\n",
      "1810\n",
      "1837\n",
      "1841\n",
      "1843\n",
      "1847\n",
      "1849\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1862\n",
      "1864\n",
      "1876\n",
      "1898\n",
      "1900\n",
      "1901\n",
      "1903\n",
      "1904\n",
      "1908\n",
      "1910\n",
      "1915\n",
      "1927\n",
      "1933\n",
      "1944\n",
      "1945\n",
      "1952\n",
      "1956\n",
      "1977\n",
      "1978\n",
      "2011\n",
      "2021\n",
      "2023\n",
      "2111\n",
      "2115\n",
      "2151\n",
      "2158\n",
      "2201\n",
      "2202\n",
      "2205\n",
      "2231\n",
      "2245\n",
      "2292\n",
      "2384\n",
      "2388\n",
      "2414\n",
      "2416\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2427\n",
      "2429\n",
      "2430\n",
      "2456\n",
      "2477\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2484\n",
      "2486\n",
      "2489\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2502\n",
      "2534\n",
      "2547\n",
      "2554\n",
      "2559\n",
      "2639\n",
      "2646\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2681\n",
      "2684\n",
      "2686\n",
      "2687\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2698\n",
      "2702\n",
      "2728\n",
      "2729\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2736\n",
      "2737\n",
      "2739\n",
      "2740\n",
      "2744\n",
      "2745\n",
      "2747\n",
      "2756\n",
      "2763\n",
      "2769\n",
      "2771\n",
      "2780\n",
      "2793\n",
      "2806\n",
      "2820\n",
      "2823\n",
      "2928\n",
      "2969\n",
      "2979\n",
      "2981\n",
      "2982\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2991\n",
      "2992\n",
      "2995\n",
      "2996\n",
      "2998\n",
      "3012\n",
      "3024\n",
      "3053\n",
      "3059\n",
      "3064\n",
      "3067\n",
      "3069\n",
      "3090\n",
      "3099\n",
      "3102\n",
      "3105\n",
      "3114\n",
      "3131\n",
      "3146\n",
      "3153\n",
      "3158\n",
      "3174\n",
      "3177\n",
      "3178\n",
      "3187\n",
      "3191\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3209\n",
      "3213\n",
      "3215\n",
      "3218\n",
      "3219\n",
      "3223\n",
      "3228\n",
      "3230\n",
      "3232\n",
      "3233\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3240\n",
      "3241\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3248\n",
      "3249\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3259\n",
      "3262\n",
      "3263\n",
      "3266\n",
      "3269\n",
      "3270\n",
      "3272\n",
      "3288\n",
      "3359\n",
      "3381\n",
      "3385\n",
      "3390\n",
      "3417\n",
      "3450\n",
      "3456\n",
      "3498\n",
      "3499\n",
      "3502\n",
      "3504\n",
      "3507\n",
      "3508\n",
      "3513\n",
      "3515\n",
      "3516\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3551\n",
      "3552\n",
      "3555\n",
      "3556\n",
      "3560\n",
      "3561\n",
      "3579\n",
      "3642\n",
      "3667\n",
      "3668\n",
      "3670\n",
      "3676\n",
      "3680\n",
      "3692\n",
      "3706\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3722\n",
      "3753\n",
      "3756\n",
      "3759\n",
      "3775\n",
      "3779\n",
      "3785\n",
      "3795\n",
      "3802\n",
      "3815\n",
      "3820\n",
      "3824\n",
      "3827\n",
      "3836\n",
      "3859\n",
      "3865\n",
      "3873\n",
      "3874\n",
      "3876\n",
      "3878\n",
      "3879\n",
      "3882\n",
      "3885\n",
      "3889\n",
      "3890\n",
      "3895\n",
      "3909\n",
      "3911\n",
      "3933\n",
      "3968\n",
      "3969\n",
      "4016\n",
      "4021\n",
      "4151\n",
      "4181\n",
      "4186\n",
      "4187\n",
      "4263\n",
      "4265\n",
      "4306\n",
      "4315\n",
      "4331\n",
      "4364\n",
      "4389\n",
      "4402\n",
      "4422\n",
      "4425\n",
      "4432\n",
      "4436\n",
      "4439\n",
      "4452\n",
      "4473\n",
      "4483\n",
      "4484\n",
      "4493\n",
      "4498\n",
      "4500\n",
      "4502\n",
      "4505\n",
      "4506\n",
      "4510\n",
      "4511\n",
      "4513\n",
      "4515\n",
      "4518\n",
      "4520\n",
      "4522\n",
      "4523\n",
      "4526\n",
      "4529\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4550\n",
      "4555\n",
      "4587\n",
      "4599\n",
      "4601\n",
      "4603\n",
      "4607\n",
      "4622\n",
      "4642\n",
      "4651\n",
      "4657\n",
      "4663\n",
      "4695\n",
      "4700\n",
      "4713\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4722\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4736\n",
      "4737\n",
      "4741\n",
      "4743\n",
      "4745\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4752\n",
      "4753\n",
      "4755\n",
      "4756\n",
      "4759\n",
      "4760\n",
      "4765\n",
      "4767\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4774\n",
      "4777\n",
      "4778\n",
      "4780\n",
      "4781\n",
      "4784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} <CIMultiDictProxy('Date': 'Sat, 12 Aug 2023 16:47:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7f5a24120fed2ae6-ORD', 'alt-svc': 'h3=\":443\"; ma=86400')>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4785\n",
      "4786\n",
      "4787\n",
      "4791\n",
      "4792\n",
      "4796\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4804\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4815\n",
      "4816\n",
      "4818\n",
      "4820\n",
      "4821\n",
      "4823\n",
      "4825\n",
      "4828\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4837\n",
      "4839\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4854\n",
      "4855\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4888\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4898\n",
      "4899\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4927\n",
      "4929\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4936\n",
      "4937\n",
      "4941\n",
      "4943\n",
      "4945\n",
      "4946\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4954\n",
      "4957\n",
      "4958\n",
      "4961\n",
      "4965\n",
      "4966\n",
      "4974\n",
      "4976\n",
      "4977\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(4852, len(sequence_data)):\n",
    "for i in has_difference:\n",
    "    print(i)\n",
    "    r = await extract_from_documents(chain=chain, documents = sequence_data[i:i+1],max_concurrency=1, use_uid=False  )\n",
    "    outputs[i]=r[0]['data']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffb81703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4852"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e76f780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97b96502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4987 records to ../llm_parsed_data/on_new_dataset(complete)/new_result_4987_V2.jsonl\n"
     ]
    }
   ],
   "source": [
    "utils.savejsonl(outputs, '../llm_parsed_data/on_new_dataset(complete)/new_result_4987_V2.jsonl')\n",
    "\n",
    "utils.savejson(outputs, '../llm_parsed_data/on_new_dataset(complete)/new_result_4987_V2.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "153b3f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field <inline-formula><tex-math notation=\"LaTeX\">Z_$f$_Z</tex-math></inline-formula> defined on a <inline-formula><tex-math notation=\"LaTeX\">Z_$d$_Z</tex-math></inline-formula>-dimensional simplicial complex <inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {K}$_Z</tex-math></inline-formula>, with <inline-formula><tex-math notation=\"LaTeX\">Z_$d \\\\leq 3$_Z</tex-math></inline-formula>. Our work revisits the seminal algorithm <italic>“PairSimplices”</italic> [31], [103] with discrete Morse theory (DMT) [34], [80], which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in <italic>“PairSimplices”</italic> [31], [103] for the fast computation of the <inline-formula><tex-math notation=\"LaTeX\">Z_$0^{th}$_Z</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">Z_$(d-1)^{th}$_Z</tex-math></inline-formula> diagrams, noted <inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {D}_{0}(f)$_Z</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {D}_{d-1}(f)$_Z</tex-math></inline-formula>. Minima-saddle persistence pairs (<inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {D}_{0}(f)$_Z</tex-math></inline-formula>) and saddle-maximum persistence pairs (<inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {D}_{d-1}(f)$_Z</tex-math></inline-formula>) are efficiently computed by processing , with a Union-Find , the unstable sets of 1-saddles and the stable sets of <inline-formula><tex-math notation=\"LaTeX\">Z_$(d-1)$_Z</tex-math></inline-formula>-saddles. We provide a detailed description of the (optional) handling of the boundary component of <inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {K}$_Z</tex-math></inline-formula> when processing <inline-formula><tex-math notation=\"LaTeX\">Z_$(d-1)$_Z</tex-math></inline-formula>-saddles. This fast pre-computation for the dimensions 0 and <inline-formula><tex-math notation=\"LaTeX\">Z_$(d-1)$_Z</tex-math></inline-formula> enables an aggressive specialization of [4] to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of <inline-formula><tex-math notation=\"LaTeX\">Z_$\\\\mathcal {D}_{1}(f)$_Z</tex-math></inline-formula>, the intermediate layer of the <italic>sandwich</italic>. Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. We also contribute a reproducible benchmark package, which exploits three-dimensional data from a public repository and compares our algorithm to a variety of publicly available implementations. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal <italic>“PairSimplices”</italic> algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output. We illustrate the utility of our contributions with an application to the fast and robust extraction of persistent 1-dimensional generators on surfaces, volume data and high-dimensional point clouds.', metadata={'source': '/Users/yameitu/Documents/Yamei/Research/2024PVIS/raw_data/1_tvcg_with_abstract_4987.json', 'seq_num': 4852})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data[4851]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e393a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvis2024",
   "language": "python",
   "name": "pvis2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
